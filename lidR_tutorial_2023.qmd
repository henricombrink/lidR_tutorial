---
title: "lidR Workshop 2023"
author: "Henri Combrink"
format:
  html:
    theme: cosmo
output:
  html: 
    number-sections: true
    number-depth: 4
    embed-resources: true
execute:
  eval: true
  include: true
  echo: true
  warning: false
  error: false
  cache: true # for faster compilation times
params:
  runpreanalysis: true #lidar download and filter step
  rundataproducts: true #lidar products generation
  set_cores: 1 # set 0 for all cores/workers or number to subtract
  transect_width: 4
---

# Setup
For this code to work correctly, please open the .Rproj file. This .qmd file should then load in Rstudio within the .Rproj file.

## Set control variables
Parameters are specified in the `YAML` preamble under `params`. If this is the first time you are running this code set `runpreanalysis` and `rundataproducts` as `true`. This will load all the data from scratch and calculate the data products.  This might take a while depending on your internet connection and computing resources. 

If you have already run the pre-analysis and data-products sections you can save time by setting as `runpreanalysis` and `rundataproducts` as `false`.

Currently `runpreanlaysis: ` is `r params$runpreanalysis` and `rundataproducts` is `r params$rundataproducts`.

The `set_cores` variable under `params` is for setting the amount of cores to use when running parallel code later in this tutorial.

The `transect_width` parameter under `params` is for setting the transect width later on in this tutorial.

##  Load packages and source functions
```{r}
#| label: load packages and functions

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  here,
  curl,
  RCurl,
  stringr,
  dplyr,
  pbmcapply,
  terra,
  lidR,
  lidRmetrics,
  future,
  sf,
  leaflet,
  ggplot2,
  patchwork,
  sjPlot,
  ggeffects,
  latex2exp,
  tidyterra,
  geosphere,
  readxl,
  knitr,
  tidyverse,
  webshot2,
  rgl,
  htmltools,
  printr,
  lme4,
  lmerTest,
  data.table,
  rmarkdown
)



# # Load packages
# require(here) # For relative directories. 
# require(curl) # url support
# require(RCurl) # HTTP / FTP Client Interface for R
# require(stringr) # All kinds of string manipulations
# require(dplyr) # The king of data wrangling packages
# require(pbmcapply) # parallel list apply with progress bar
# require(terra) # modern spatial package
# 
# require(lidR) # Lidar workhorse
# require(lidRmetrics) # For additional lidar forest metrics
# require(future) # parallel processing 
# 
# require(sf) # spatial package
# require(leaflet) # plotting package with open street maps
# require(ggplot2) # plotting workhorse
# require(patchwork) # putting many plots together
# require(sjPlot) # plotting model outputs
# require(ggeffects) # generate model predictions
# require(latex2exp) # print latex in plots ect.
# require(tidyterra) # visualizing 'terra' objects with 'ggplot2
# #require(ggmap) # OSM maps ect.
# #require(osmdata)
# require(geosphere) # compute various aspects of distance, direction, area, etc. for geographic (geodetic) coordinates
# 
# require(readxl) # import excel files into R
# require(knitr) # for compiling rmarkdown 
# require(tidyverse) # lots of usefull packages
# require(webshot2) # take screenshots of webpages
# 
# require(rgl)
rgl::setupKnitr()
knitr::knit_hooks$set(webgl = hook_webgl)
# 
# require(htmltools) # Tools for HTML generation and output.
# require(printr) # printing R content
# 
# require(lme4) # mixed models
# require(lmerTest) # p-values in summery table

#packages_required <-
#subset(data.frame(sessioninfo::package_info()), attached==TRUE, c(package, loadedversion))

# Source functions
source(here::here("functions/rfunction_plot_crossection.R"))
source(here::here("functions/rfunction_treedetect_lmf_varw_1.R"))
#source(here::here("functions/rfunction_load_r_packages.R"))


#####################
# Control variables
#####################
#set amount of cores for parallel processing
cores <- length(future::availableWorkers()) - params$set_cores

```

# Introduction

This tutorial provides a brief overview of the capabilities of the `lidR` package for processing point cloud data. We briefly explore how to download data from the internet for specific study site locations. We then perform some basic point cloud processing steps. Next, we calculate some basic forest metrics using the `lidRmetrics`package.

Once we have completed all the data generating steps, we will merge the point cloud products with a data set and perform some basic statistical analysis.

The study sites and data originate from a study performed by a SNRE master student. The study sites are located in 3 different forest treatments, control, thinned and fuel reduction areas. The student wanted to know if the forest treatments would have any effect on the sound attenuation of the endangered [Mount Graham Red Squirrel](https://en.wikipedia.org/wiki/Mount_Graham_red_squirrel). 

To this end the student played a artificial recording consisting of a compilation of the territorial calls of several Mount Graham Red Squirrels at 30 sites distributed among the forest treatments. Recording devices where located at 1, 10, 20 and 40 meters from the source in all 4 cardinal directions. These sound recordings was processed and the average amplitude of each recording for each site is reported. However, the reader should note that although tutorial uses an actual study as the foundation of the tutorial, the specific locations of the study sites and the sound attenuation data have been simulated from the original data set for the purposes of this tutorial. No inference should be drawn from the data or subsequent results.

![Mount Graham Red Squirrel](https://upload.wikimedia.org/wikipedia/commons/3/38/Mt._Graham_Red_Squirrel.jpg){#fig-mgrsqrl}





# Lidar in R

For this analysis we rely heavily on the `lidR` package in R. You can find the `lidRbook` at (<https://r-lidar.github.io/lidRbook/index.html>).

```{r}
#| label: lidrbook
#| echo: false

# make folder if required
if(!dir.exists(here::here("temp"))){
  dir.create(here::here("temp"))
}

webshot("https://r-lidar.github.io/lidRbook/index.html",
        here::here("temp","lidRbook_index.png"),
        cliprect = "viewport",
        quiet = TRUE) %>%
resize("75%")
```

# Download and format data

## Download the laz files from the USGS server

### Find some lidar data

You can search for lidar data on the USGS LidarExplorer page: <https://apps.nationalmap.gov/lidar-explorer/#/>

We select the [AZ\_VerdeKaibab_2018_B18](https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/) data set that was collected over the Mount Graham area during 2018. The [parent directory](https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/) contains metadata, .jpg tiles and the .laz files.

```{r}
#| label: usgs_rockyweb
#| echo: false

webshot2::webshot("https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/",
        here::here("temp","rockyweb.png"),
        cliprect = "viewport",
        delay = 1,
        quiet = TRUE) %>%
resize("75%")

```

### Retrieve the download list

Next we want to retrieve a list of the url locations that we would like to download. To achieve this we point R to the [0_file_download_links.txt]("https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/0_file_download_links.txt") file. Let's download it, save it to a file and have a look at the output.

```{r}
#| label: lidar_link_location

# Link to text file with a URL for each available tile
lazfiles <- "https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/0_file_download_links.txt"

# make folder if required
if(!dir.exists(here::here("data_output", "pointcloud", "verdekaibab_2018"))){
  dir.create(here::here("data_output", "pointcloud", "verdekaibab_2018"))
}

# Download file from the internet 
download.file(lazfiles, here::here("data_output", "pointcloud", "verdekaibab_2018", "laslist.txt"))
head(list.files(here::here("data_output", "pointcloud", "verdekaibab_2018")))

```

### Format the download list

Next we pull out the individual file names.

```{r}
#| label: download list

# Read in text file
laslist <- read.csv(here::here("data_output", "pointcloud", "verdekaibab_2018", "laslist.txt"),
                    sep = "\n")

# Transform to a list
laslist <- as.list(laslist[,1])

# Strip out only the file name
laslist_truncated <-
laslist %>%
  str_trunc(47, "left", ellipsis = "")
```

Let's have a look at the first 2 truncated file names. This is the filename of each lidar tile.

```{r}
#| label: download list_2

head(laslist_truncated, 2)
```

Let's have a look at first 2 url's in the download list. This is the url where each lidar tile can be downloaded.

```{r}
#| label: download list_3

head(laslist)[1:2] #have a look

```

```{r}
#| label: download list cleanup
#| echo: false
#| include: false

rm(lazfiles); gc() #clean up

```

## Generate a vector file for the sample sites

Here we sort out some projection problems, generate different coordinates for different projections and generate a shapefile for later use. These are our sampling sites plotted over the general area.

```{r}
#| label: convert soundsite coordinates

# Read in the text file
soundsites <- read.csv(here::here("data","playbacks_plots_fuzzy.csv"), header = T)

# Transform it to a vector file using the terra package
soundsites <- vect(soundsites,
                   geom = c("Northing","Easting"),
                   crs = "epsg:26912")
soundsites$x_26912 <- geom(soundsites)[,"x"]
soundsites$y_26912 <- geom(soundsites)[,"y"]

# Change projection to match lidar data
soundsites <- project(soundsites, "epsg:6350") #NAD83(2011)/ Conus Albers
soundsites$x_6350 <- geom(soundsites)[,"x"]
soundsites$y_6350 <- geom(soundsites)[,"y"]

# Change to WGS84 for interaction with other functions later
soundsites_wgs84 <- project(soundsites, "EPSG:4326") # Un-projected
soundsites$x_wgs84 <- geom(soundsites_wgs84)[,"x"]
soundsites$y_wgs84 <- geom(soundsites_wgs84)[,"y"]

# Write to disk for later use
write.csv(soundsites, here::here("data_output","playbacks_plots_coords.csv"))

# Write to disk as .shp
if (file.exists(here::here("data_output", "gis", "soundsites.shp"))){
file.remove(here::here("data_output", "gis", "soundsites.shp")) #writeVector does not want to overwrite an existing file for some reason, thus delete first if file already exists.
#print("Existing soundsites deleted")
}
writeVector(soundsites,
            here("data_output", "gis", "soundsites.shp"))

```

```{r}
#| label: plot soundsites
#| fig-cap: "Site locations in the Mt. Graham area."

# Let's have a look the plot locations

# plot(soundsites) # NAD83(2011)/ Conus Albers projected locations

leaflet(soundsites_wgs84) %>%
  addTiles() %>%
  setView(lng = mean(geom(soundsites_wgs84)[,"x"]),
          lat = mean(geom(soundsites_wgs84)[,"y"]),
          zoom = 13) %>%
  addCircleMarkers(lng = geom(soundsites_wgs84)[,"x"],
                   lat = geom(soundsites_wgs84)[,"y"],
                   color = "red",
                   radius = 5)

```

## Download lidar tiles

If we have a look at the [tiles of the entire lidar dataset](https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/metadata/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/shapefiles/USGS_Kaibab_QL1_Albers_1K_Tile_Index_with_Buffer_Tiles.shp), we can see it covers a substantial area. We are only interested in a small area. Since we have limited system resources it is a good idea to subset the data, so we will download only the lidar tiles that overlap with our sites.

```{r}
#| label: intersect lidar and download

# Spatial extent of the entire lidar data set
extensions <- c(".shp",".prj",".dbf",".shx")

# Download url
tile_index <- "https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/metadata/AZ_VerdeKaibab_2018_B18/AZ_VerdeKaibab_B1_2018/shapefiles/USGS_Kaibab_QL1_Albers_1K_Tile_Index_with_Buffer_Tiles"

# download all the shapefile extensions
for(i in 1:length(extensions)){
download.file(paste0(tile_index, extensions[i]), here::here("data_output", "gis", paste0("USGS_Kaibab_QL1_Albers_1K_Tile_Index_with_Buffer_Tiles", extensions[i])), quiet = TRUE)
}

#terra::vect
lidar_tiles <- terra::vect(here::here("data_output/gis/USGS_Kaibab_QL1_Albers_1K_Tile_Index_with_Buffer_Tiles.shp"))

lidar_tiles_wgs84 <- project(lidar_tiles, "EPSG:4326") # WGS84

# clean up
rm(tile_index); gc()
```

```{r}
#| label: lidar tiles
#| eval: true
#| echo: false
#| include: true
#| fig-cap: "Larger lidar campaign with our study sites."

# Let's have a look
# plot(lidar_tiles) # projected tiles

# Plot larger lidar campaign with our study sites. Scroll down to the south-east.
leaflet(lidar_tiles_wgs84) %>%
  addTiles() %>%
  setView(lng = mean(geom(lidar_tiles_wgs84)[,"x"]),
          lat = mean(geom(lidar_tiles_wgs84)[,"y"]),
          zoom = 6) %>%
  addPolygons() %>%
  addCircleMarkers(lng = geom(soundsites_wgs84)[,"x"],
                   lat = geom(soundsites_wgs84)[,"y"],
                   color = "red",
                   radius = 5)


```

We buffer each of our study site point coordinates by 100 meters with the `terra:buffer` function. This ensures that we have sufficient area around each point and would allow us to also select neighboring tiles where a site is close to the edge of two lidar tiles. This help prevent edge effects when analyzing the data later on. It is a good idea to always select a larger area to prevent edge effects.

We then intersect `lidar_tiles` with the buffered tiles (`soundsites_buffer_100`) to derive a subset of the applicable lidar tiles.

```{r}
#| label:  intersect lidar and download 2


# Give sound sites a 100m buffer
soundsites_buffer_100 <- terra::buffer(soundsites, 100)

if (file.exists(here::here("data_output",
                 "gis",
                 "soundsites_buffer_100.shp"))){
file.remove(here::here("data_output",
                 "gis",
                 "soundsites_buffer_100.shp"))
}

writeVector(soundsites_buffer_100,
            here::here("data_output",
                 "gis",
                 "soundsites_buffer_100.shp"))

# select the lidar tiles that correspond to the buffered soundsites
lidar_tiles_subset <- lidar_tiles[soundsites_buffer_100]
#plot(lidar_tiles_subset) # Lets have a look

lidar_tiles_subset_wgs84 <- project(lidar_tiles_subset, "EPSG:4326") # to WGS 84

```

```{r}
#| label: plot intesected tiles
#| fig-cap: "Intersected tiles."

leaflet(lidar_tiles_subset_wgs84) %>%
  addTiles() %>%
  setView(lng = mean(geom(soundsites_wgs84)[,"x"]),
          lat = mean(geom(soundsites_wgs84)[,"y"]),
          zoom = 13) %>%
  addPolygons() %>%
  addCircleMarkers(lng = geom(soundsites_wgs84)[,"x"],
                   lat = geom(soundsites_wgs84)[,"y"],
                   color = "red",
                   radius = 5) 


```

```{r}
#| label: clean_lidar_intersect
#| eval: TRUE
#| echo: FALSE
#| include: FALSE

rm(lidar_tiles); gc(verbose = FALSE) # Clean up
```

In this next code chunk we are going to download the lidar data. By default I set `runpreanalysis` to `false`. If you want to download the data, set `runpreanalysis` to `true` under `params` in the `YAML` preamble. It is currently set as `r params$runpreanalysis`.

This code chuck terminates in a parallel list apply function with a progress bar to speed up downloads and see progress. If it does not work for you for some reason you can replace `pbmclapply` with `lapply`. If the progress bar does not work you can navigate to the download location (`r here::here("data_output", "pointcloud", "verdekaibab_2018")`) to see if the download is in progress. The download will take about 7 minutes depending on your connection.

```{r}
#| label:  intersect lidar and download 3
#| eval: !expr params$runpreanalysis
#| echo: true
#| include: true

# Download lidar data
# list of lidar tiles intersecting points from with soundsites

soundtiles <- lidar_tiles_subset$TileID # These are the tiles that contain our points. Use it below.

# Interrogate tiles against laslist
laslistdownload <- list() # generate an empty list that we will populate next
for(i in 1:length(soundtiles)){
temp <- str_match(laslist, soundtiles[[i]])
laslistdownload[i] <- laslist[[which(!is.na(temp))]]
rm(temp)
}

# set download timeout to be longer than 60 seconds
options(timeout = max(3600, getOption("timeout")))

# This a a parallel list apply function with a progress bar to speed up downloads and see progress. If it does not work for you for some reason you can replace pbmclapply with lapply
pbmclapply(1:length(laslistdownload), function(x){
 url = paste0(laslistdownload[[x]])
 download.file(
  url,
  here::here("data_output", "pointcloud", "verdekaibab_2018", paste0("USGS_LPC_AZ_VerdeKaibab_2018_B18_", soundtiles[[x]], ".laz")))
 },# FN END
mc.cores = getOption("mc.cores", cores))


rm(laslistdownload, soundtiles); gc() # Clean up

```

## Reduce tiles

Now we can reduce the lidar chunk size for lower memory overhead later.

The `LAScatalog processing engine` is a very powerful tool with many options accessed with `lidR::catalog_*`. See the [LidR book](https://r-lidar.github.io/lidRbook/engine.html) or the help documentation for details (e.g. `?catalog_retile()`). 

When loading a `LAScatalog` with `readLAScatalog` you can filter out classified returns with the `filter` option in the `readLAScatalog` function. Here we filter out noise (i.e. points classified as "low" and "high"). See sections 2.11 and 2.1.2 in the [lidR book](https://r-lidar.github.io/lidRbook/engine.html) for options and syntax.

The `las_check()` function performs an inspection of LAScatalog objects for file consistency.

```{r reduce tile size, eval=runpreanalysis}
#| label: reduce the tiles and make them smaller
#| eval: !expr params$runpreanalysis

# Read the lidar tiles associated with the soundsites as a catalog using the lidR package
ctg <- readLAScatalog(here::here("data_output",
                                 "pointcloud",
                                 "verdekaibab_2018"),
                          filter = "-drop_class 7 18")

# validation
las_check(ctg)


ctg_crs <- sf::st_crs(ctg) # retrieve crs of catalog

plot(ctg, chunk = T)

# Define options that will apply to ctg
opt_chunk_buffer(ctg) <- 15 # meters
opt_chunk_size(ctg) <- 250 # meters

plot(ctg, chunk = T)

```



```{r}
#| label: index reduced catalog
#| eval: !expr params$runpreanalysis

# Create indexation of the points within the laz files (lax file) 
# This speeds up the read times for later 

lidR:::catalog_laxindex(ctg)

```


# Data products

Begin here if all the lidar data is already downloaded. All chunks above will be skipped if `runpreanalysis` is `false`. It is currently set as: `r params$runpreanalysis`

Set `rundataproducts` as `true` under `params` in the `YAML` preamble to run the data product section. It is currently set as `r params$rundataproducts`.

## Load catalog and roi

Load the reduced lidar catalog with the `lirR::readLAS` function. 

```{r}
#| label: load catalog and roi
#| eval: !expr params$rundataproducts


#readLAS(filter = "-help") #Filter help

ctg <- readLAScatalog(
  here::here("data_output", "pointcloud", "verdekaibab_2018"),
  filter = "-drop_class 7 18 -drop_z_below 0") # filter "low" and "high" noise)

# opt_output_files(ctg) <- paste0(here::here("data_output",
#                                           "pointcloud",
#                                           "verdekaibab_2018_transects"),
#                                           "/{XLEFT}_{YBOTTOM}")
# opt_output_files(output) <- "" # clear 
opt_chunk_buffer(ctg) <- 15 # meters
opt_chunk_size(ctg) <- 250 # meters


# Load our study plot info
soundsites_shp <- terra::vect(here::here("data_output", "gis", "soundsites.shp"))

# Load the buffered study plot info
buffer_shp <- terra::vect(here("data_output",
                               "gis",
                               "soundsites_buffer_100.shp"))

```

The chuck below illustrates how you could select a circular region of interest around a point. This example will extract a point cloud around each point with a radius of 100m. We won't run this example.

```{r clip_circle}
#| label: example of clip_circle
#| eval: false
#| echo: true
#| include: false


# Generate a region of interest around each site. Here we used a radius of 100 meters.

roi <- clip_circle(ctg,
                   soundsites_shp$x_6350,
                   soundsites_shp$y_6350,
                   radius = 100)


```

## Create sample transects

Here we generate sample transects in each plot. The transects are generated in each cardinal direction between two points. The points are at 1, 10, 20, and 40 meters. The transects are `r params$transect_width` meters wide.

We then use the `geosphere` package to generate WGS84 un-projected coordinates for each distance in each cardinal direction. See `?geosphere::destPoint` for details.

```{r}
#| label: create sample transects
#| eval: !expr params$rundataproducts

# generate sample points for transects
receiver_loc <- expand.grid(c(0,90,180,270), c(1,10,20,40), soundsites_shp$Plot)
names(receiver_loc) <- c("bearing","distance", "plot")
receiver_loc$plot_x_wgs84 <- rep(soundsites_shp$x_wgs84, each=16)
receiver_loc$plot_y_wgs84 <- rep(soundsites_shp$y_wgs84, each=16)

receivers <-
lapply(1:nrow(receiver_loc), function(x){
  geosphere::destPoint(p = cbind(receiver_loc[x,"plot_x_wgs84"], receiver_loc[x,"plot_y_wgs84"]),
                       b = receiver_loc[x,"bearing"],
                       d = receiver_loc[x,"distance"])
})

receivers <- as.data.frame(do.call(rbind,receivers))
receiver_loc$rec_x_wgs84 <- receivers$lon
receiver_loc$rec_y_wgs84 <- receivers$lat

# Generate the treatment variable
treatment <- rep(soundsites_shp$Treatment, each=16)

receiver_loc$treatment <- treatment

head(receiver_loc,16)
```

```{r}
#| label: Generate source receiver vector
#| eval: !expr params$rundataproducts

# Generate receiver vector
receiver_loc_vect_wgs84 <- terra::vect(receiver_loc, geom = c("rec_x_wgs84","rec_y_wgs84"), crs = "epsg:4326")

#head(geom(receiver_loc_vect))
#crs(receiver_loc_vect)

#receiver_loc_vect <-  project(receiver_loc_vect_wgs84, "EPSG:6350")
receiver_loc_vect <-  project(receiver_loc_vect_wgs84, crs(ctg))
#crs(receiver_loc_vect)

# Generate source vector
source_loc_vect_wgs84 <- terra::vect(receiver_loc, geom = c("plot_x_wgs84","plot_y_wgs84"), crs = "epsg:4326")

#head(geom(source_loc_vect))
#crs(source_loc_vect)

#source_loc_vect <-  project(source_loc_vect_wgs84, "EPSG:6350")
source_loc_vect <-  project(source_loc_vect_wgs84, crs(ctg))

#crs(source_loc_vect)
#head(geom(source_loc_vect))

# Plot example of sample points for transects
plot(receiver_loc_vect[receiver_loc_vect$plot == "CP019",])
plot(source_loc_vect[source_loc_vect$plot == "CP019",], col = "red", add = T)

# Points and lidar tiles
pal <- colorFactor(c("yellow","green","navy", "red"),
                   domain = unique(receiver_loc_vect$distance))

leaflet(receiver_loc_vect_wgs84) %>%
  addTiles() %>%
  addCircleMarkers(
    color = ~pal(distance),
    stroke = FALSE,
    fillOpacity = 0.5) %>%
  addMarkers(source_loc_vect_wgs84,
             label = ~ treatment,
             lng = geom(source_loc_vect_wgs84)[,"x"],
             lat = geom(source_loc_vect_wgs84)[,"y"])
# Zoom in to see the details around each site

```

## Generate sample transects

Next we generate the sample transects. We achieve this with the `lidR::clip_transect` function. The code below performs a list apply function and subsets a rectangle between the plot center and a points for each cardinal direction at 1, 10, 20 and 40 meters. All resulting point clouds are written to disk.

```{r}
#| label: soundsites transects
#| eval: !expr params$rundataproducts

soundsites_transects <- 
pbmclapply(1:nrow(receiver_loc), function(rcvr){ #START function
#lapply(1:nrow(receiver_loc), function(rcvr){ #use if pbmclapply does not work
# clip the transect  
  transect <-  
  clip_transect(ctg,
                p1 = cbind(geom(source_loc_vect)[rcvr,"x"], geom(source_loc_vect)[rcvr,"y"]),
                p2 = cbind(geom(receiver_loc_vect)[rcvr,"x"], geom(receiver_loc_vect)[rcvr,"y"]),
                width = params$transect_width)

# Create dir
  if(!dir.exists(here::here("data_output",
                            "pointcloud",
                            "verdekaibab_2018_transects"))) dir.create(here::here("data_output","pointcloud","verdekaibab_2018_transects"))

# Write to disk  
  writeLAS(transect,
           here::here("data_output", "pointcloud", "verdekaibab_2018_transects",
                       paste0(receiver_loc[rcvr,"plot"],
                       "_",receiver_loc[rcvr,"distance"],
                       "_", receiver_loc[rcvr,"bearing"],
                       ".las")))
# return transect
  transect
  
}, #END function

# set up parallel processing
mc.cores = getOption("mc.cores", cores)
) #END pbmclapply

# clean up
rm(recievers, treatment); gc()


```


# Generate lidar derived metrics

Next we generate a data frame to hold all the lidar derived metrics.

```{r magnumopus, eval=rundataproducts, echo=TRUE, include=TRUE}
#| label: magnumopus
#| eval: !expr params$rundataproducts

# Big data frame to hold all the generated variables per plot per distance per direction
magnumopus <- receiver_loc
head(magnumopus)

```

We use a list apply function to run through each transect; plot(1:30); distance (1,10,20,40); direction (N, E, S, W). We use the `lidR::rasterize_terrain` function with `soundsite_transects` as roi (region of interest) to calculate each variable. See `?rasterize_terrain` for more information.

## Digital terrain model

Generate the digital terrain model and look at an example.

```{r}
#| label: dtm
#| eval: !expr params$rundataproducts


dtm_tin <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
  rasterize_terrain(soundsites_transects[[rcvr]], res = 0.25, algorithm = tin())
})

# Let's have a look at all the stuff we created using plot 16 as a random example
receiver_loc[16,]
plot(dtm_tin[[16]])
plot(receiver_loc_vect[16], add = T)
plot(source_loc_vect[16], add = T)

```

## mls elevation for each plot and transect point

Next we generate a elevation (meters above sea level) layer for each roi

```{r}
#| label: elevation of plots
#| eval: !expr params$rundataproducts


# Extract average msl per transect
elevation <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
  ras <- dtm_tin[[rcvr]]
  source <- source_loc_vect[rcvr]
  receiver <- receiver_loc_vect[rcvr]
  source.pts <- data.frame(x = geom(source)[,c("x")],
                         y = geom(source)[,c("y")])
  receiver.pts <- data.frame(x = geom(receiver)[,c("x")],
                           y = geom(receiver)[,c("y")])
  Z_source <- terra::extract(ras, source.pts)
  Z_receiver <- terra::extract(ras, receiver.pts)
  list(Z_source = Z_source[,"Z"], 
     Z_reciever = Z_receiver[,"Z"],
     Z_sr_diff = Z_receiver[,"Z"] - Z_source[,"Z"])
})

elevation <- data.table::rbindlist(elevation, fill=TRUE) # This transforms the named list to a data.table

# Merge with magnumopus
magnumopus <- cbind(magnumopus, elevation)

# Let's have a look
head(elevation)

# clean up
rm(elevation); gc()
```

## Slope and aspect for each transect

Next we calculate the average slope and aspect of each transect.

```{r}
#| label: slope and aspect
#| eval: !expr params$rundataproducts

# Slope and aspect
dtm_prod <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
    terrain(dtm_tin[[rcvr]], v = c("slope", "aspect"), unit = "degrees")
})

# Let's see a random example
plot(dtm_prod[[16]])

# Extract average slope and aspect per transect
slope_aspect <-
  lapply(1:length(dtm_prod), function(x){
    dt <- as.data.frame(dtm_prod[[x]])
    slope <- mean(dt$slope, na.rm = T)
    aspect <- mean(dt$aspect, na.rm = T)
    list(slope = slope, aspect = aspect)
  })

slope_aspect <- data.table::rbindlist(slope_aspect, fill=TRUE) #list to data.table

# Merge with magnumopus
magnumopus <- cbind(magnumopus, slope_aspect)

# Group aspect as.numeric
magnumopus$aspect_card_num <- ifelse(magnumopus$aspect >= 45 & magnumopus$aspect < 135, 1,
       ifelse(magnumopus$aspect >= 135 & magnumopus$aspect < 225, 2,
              ifelse(magnumopus$aspect >= 225 & magnumopus$aspect < 315, 3, 4
)))
magnumopus$aspect_card_num <- as.factor(magnumopus$aspect_card_num)


# clean up
rm(slope_aspect); gc()
```

## Surface area of each transect.

Next we calculate the surface area of each transect. We will use this later on to scale some of the metrics to account for the variable transect sizes.

```{r}
#| label: area of each transect
#| eval: !expr params$rundataproducts

# Area of each transect
area <-
  lapply(1:length(dtm_prod), function(x){
  terra::expanse(dtm_prod[[x]])[1,2]   
})

# Merge with magnumopus
area <- as.data.frame(do.call(rbind, area))
names(area) <- "transect_area"
magnumopus$transect_area <- area$transect_area

head(area)

# clean up
rm(area); gc()
```

## Normalize the elevation

This sets ground points to zero and essentially makes the surface flat.

```{r}
#| label: normalize height
#| eval: !expr params$rundataproducts

norm_height <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
    normalize_height(soundsites_transects[[rcvr]], knnidw())
})
# Keep for later use
```

```{r}
#| label: normalize height to disk
#| eval: !expr params$rundataproducts
#| include: false

dir.create(
here::here("data_output","pointcloud","verdekaibab_2018_transects_norm")
)

# Write las to file
lapply(1:length(norm_height), function(x){
  writeLAS(norm_height[[x]], here::here("data_output","pointcloud","verdekaibab_2018_transects_norm",
            paste0(receiver_loc[x,"plot"],
                   "_",receiver_loc[x,"distance"],
                   "_", receiver_loc[x,"bearing"],
                   ".las")))
})

# Keep for later use

```

## List of metrics

This is a table of potential metrics from the `lidRmetrics` package. See <https://github.com/ptompalski/lidRmetrics>

```{r}
#| label: List of lidRmetrics metrics
#| eval: !expr params$rundataproducts

download.file(
  "https://github.com/ptompalski/lidRmetrics/blob/main/list_of_metrics.xlsx?raw=TRUE",
  here::here("temp", "list_of_metrics.xlsx"))

mtrxs <- readxl::read_excel(here::here("temp", "list_of_metrics.xlsx"))

options(knitr.kable.NA = '')

kable_out <- knitr::kable(mtrxs, "html")
readr::write_file(kable_out, here::here("temp", "kable_out.html"))

```

```{r}
#| label: lidRmetrics variable table
#| eval: !expr params$rundataproducts

htmltools::includeHTML(here::here("temp", "kable_out.html"))

```

## Height distribution metrics

Metrics using the `lidRmetrics::metrics_basic` function. See `?metrics_basic` for details.

```{r}
#| label: metrics basic
#| eval: !expr params$rundataproducts

help(metrics_basic)

```

```{r}
#| label: mean vegetation height
#| eval: !expr params$rundataproducts

# Variables: n, zmax, zmin, zmean, zsd, zcv, zskew, zkurt 

met_basic <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_basic(Z))
})
met_basic <- data.table::rbindlist(met_basic, fill=TRUE)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_basic)

head(met_basic)

# clean up
rm(met_basic); gc()
```

## Height percentiles

```{r}
#| label: metrics_percentiles
#| eval: !expr params$rundataproducts

help(metrics_percentiles)

```

```{r}
#| label: elevation percentiles
#| eval: !expr params$rundataproducts

#Variables: zq1, zq5, …, zq95, zq99 

met_percentiles <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_percentiles(Z))
})
met_percentiles <- data.table::rbindlist(met_percentiles, fill=TRUE)
head(met_percentiles)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_percentiles)

# clean up
rm(met_percentiles); gc()
```

## Heighs above threshold

Calculates percentage of points above specified threshold heights (default = c(2, 5)) and mean height.

```{r metrics_percabove}
#| label: metrics_percabove
#| eval: !expr params$rundataproducts

help(metrics_percabove)

```

```{r percentage of returns above a threshold}
#| label: percentage of returns above a threshold
#| eval: !expr params$rundataproducts

# Variables: pzabovemean, pzabove2, pzabove5 

met_percabove <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_percabove(Z))
})
met_percabove <- data.table::rbindlist(met_percabove, fill=TRUE)
head(met_percabove)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_percabove)

# clean up
rm(met_percabove); gc()

```

## Proportion of returns between specified elevation intervals

Percentage of points calculated for a set of horizontal layers.

```{r metrics_interval}
#| label: metrics_interval
#| eval: !expr params$rundataproducts

help(metrics_interval)

```

```{r Proportion of returns}
#| label: proportion of returns
#| eval: !expr params$rundataproducts

# Variables: pz\_below\_0, pz\_0.0.15, pz\_0.15.2, pz\_2.5, pz\_5.10, pz\_10.20, pz\_20.30, pz\_above\_30 

met_pzinterval <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]],
                  ~metrics_interval(Z, zintervals = c(0, 0.15, 2, 5, 10, 20, 30)))
})
met_pzinterval <- data.table::rbindlist(met_pzinterval, fill=TRUE)
head(met_pzinterval)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_pzinterval)

# clean up
rm(met_pzinterval); gc()
```

## Dispersion

### CRR definition

Canopy Relief-Ratio (CRR) is a quantitative descriptor of the relative shape of the canopy from altimetry observation (Parker and Russ, 2004; Pike and Wilson, 1997), defined as mean height returns minus the minimum height divided by the maximum height minus the minimum height. This ratio reflects the degree to which canopy surfaces are in the upper (\> 0.5) or in the lower (\< 0.5) portions of the height range.

Reference

Parker, G. G., & Russ, M. E. (2004). The canopy surface and stand development: assessing forest canopy structure and complexity with near-surface altimetry. Forest Ecology and Management, 189(1-3), 307-315.

Reply: Not the shape of trees, it is the variation of the canopy (relative surface morphology). The higher the CRR the more local variation occurs in the canopy (eg., open canopy) whereas small values represent less variation (eg. closed or uniform age canopy). Because of all of the different possible canopy configurations and the influence of the defined scale, there is no single interpretation of a given value thus, CRR is a relative measure of canopy variation and technically represents rugosity.

This metric is just a repackaging of Pike's geomorphometric Surface Relief Ratio (SSR) and is derived such: (mean(x) - min(x)) / (max(x) - min(x)) where; x is a matrix of some NxN window (in the case of CRR, canopy heights).

### VCI

VCI describes the evenness of the vertical distribution of the points across the canopy layers (Pearse et al., 2018; van Ewijk et al., 2011). The canopy is divided into H layers of a certain bin height, which we set to 1 m as was proposed by van Ewijk et al. (2011) and Pearse et al. (2018). H is a constant value for each test site and was chosen such that the top point in the canopy height model lies in the uppermost layer. pi corresponds to the relative abundance of points in height layer i with regard to the total number of points in the respective height column. VCI is in range \[0,1\] where VCI=1 signifies an even distribution of the points across the canopy height and decreasing values state an increasingly un-even distribution (van Ewijk et al., 2011).

```{r}
#| label: metrics_dispersion
#| eval: !expr params$rundataproducts

help(metrics_dispersion)
```

```{r}
#| label: LAS-derived indices
#| eval: !expr params$rundataproducts

# Variables: ziqr, zMADmean, zMADmedian, CRR, zentropy, VCI

met_dispersion <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_dispersion(Z))
})
met_dispersion <- data.table::rbindlist(met_dispersion, fill=TRUE)
head(met_dispersion)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_dispersion)

rm(met_dispersion, met_dispersion_ng); gc()
```

## Canopy density

Cumulative distribution of point heights in interval count.

```{r}
#| label: metrics_canopydensity
#| eval: !expr params$rundataproducts

help(metrics_canopydensity)
```

```{r}
#| label: canopy density
#| eval: !expr params$rundataproducts

# Variables: zpcum1, zpcum2,…, zpcum8, zpcum9 

met_canopydensity <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_canopydensity(Z))
})
met_canopydensity <- data.table::rbindlist(met_canopydensity, fill=TRUE)
head(met_canopydensity)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_canopydensity)

# clean up
rm(met_canopydensity); gc()
```

## Metrics based on the leaf area density

Metrics based on the leaf area density. `lidR::LAD()` used to calculate the leaf area density.

```{r}
#| label: metrics_lad
#| eval: !expr params$rundataproducts

help(metrics_lad)

```

```{r}
#| label: leaf area density
#| eval: !expr params$rundataproducts

# Variables: lad\_max, lad\_mean, lad\_cv, lad\_min, lai 

met_lad <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_lad(Z))
})
met_lad <- data.table::rbindlist(met_lad, fill=TRUE)

# remove nas
met_lad <- 
met_lad %>%
  mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x)) %>%
  mutate_if(is.numeric, function(x) ifelse(is.na(x), 0, x))

head(met_lad)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_lad[,"lai"])

rm(met_lad); gc()
```

## Kernel density estimation

Kernel density estimation applied to the distribution of point cloud elevation (Z). KDE allows to create a probability density function (using a Guassian kernel). The density function is then used to detect peaks (function maxima). Based on similar metric available in Fusion (see references), with significant differences in the list of output statistics as well as the default bandwidth used when estimating kernel density.

```{r}
#| label: metrics_kde
#| eval: !expr params$rundataproducts

help(metrics_kde)
```

```{r}
#| label: kernel density estimation
#| eval: !expr params$rundataproducts

# Variables:
# kde\_peaks\_count, kde\_peaks\_elev, kde\_peaks\_value 
# see McGaughey, R.J., 2021. FUSION/LDV: Software for LIDAR Data Analysis and Visualization

met_kde <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_kde(Z, zmin = 0))
})
met_kde <- data.table::rbindlist(met_kde, fill=TRUE)
head(met_kde)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_kde)

rm(met_kde); gc()
```

## Height of median energy

The function provided here aims to mimic the HOME metric, which is a metric typically used with full-waveform lidar. HOME is calculated by identifying an elevation that splits the total intensity into two equal parts. Function is based on a similar metric implemented in [LAStools](https://lastools.github.io/).

```{r}
#| label: metrics_HOME
#| eval: !expr params$rundataproducts

help(metrics_HOME)
```

```{r}
#| label: height of median energy
#| eval: !expr params$rundataproducts

# Variable: HOME

met_home <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_HOME(Z, Intensity, zmin = 0))
})
met_home <- as.data.frame(do.call(rbind, met_home))
names(met_home) <- "HOME"
head(met_home)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_home)

rm(met_home); gc()
```

## Voxels

A set of metrics calculated in a voxel space, designed to be used within the grid_metrics or cloud_metrics function from the lidR package. For convenience, a point cloud is converted to a voxel space on the fly, without the need of using additional processing steps. Note, that because of the additional computation required to convert a point cloud to voxels, calculating voxel-based metrics is markedly slower than other metrics\_\* functions.

For info on some of the variables calculated see: Lefsky, M. A., Cohen, W. B., Acker, S. A., Parker, G. G., Spies, T. A., & Harding, D. (1999). Lidar Remote Sensing of the Canopy Structure and Biophysical Properties of Douglas-Fir Western Hemlock Forests. Remote Sensing of Environment, 70(3), 339--361.

```{r}
#| label: metrics_voxels
#| eval: !expr params$rundataproducts

help(metrics_voxels)

```

```{r}
#| label: voxels
#| eval: !expr params$rundataproducts

# Variables:
# vn, vFRall, vFRcanopy, vzrumple,  vzsd, vzcv, OpenGapSpace,
# ClosedGapSpace, Euphotic, Oligophotic  

met_voxels <-
lapply(1:nrow(receiver_loc), function(rcvr){
    cloud_metrics(norm_height[[rcvr]], ~metrics_voxels(x = X, y = Y, z = Z))
})
met_voxels <- data.table::rbindlist(met_voxels, fill=TRUE)
head(met_voxels)

# combine with main dt 
magnumopus <-
  cbind(magnumopus, met_voxels)

# clean up
rm(met_voxels); gc()
```

## Canopy height and digital surface model

The difference between the canopy height (CHM) and the digital surface (DSM) models are that the `dsm` is generated with the elevation (above sea level) data and the `chm` is generated with the normalized elevation data.

```{r}
#| label: canopy height model
#| eval: !expr params$rundataproducts

# msl elevation
dsm <- lapply(1:nrow(receiver_loc), function(rcvr){
  rasterize_canopy(soundsites_transects[[rcvr]], res = 0.25, algorithm = p2r(subcircle = 0.15))
})


# normalized heights
chm <- lapply(1:nrow(receiver_loc), function(rcvr){
  rasterize_canopy(norm_height[[rcvr]], res = 0.25, algorithm = p2r(subcircle = 0.15))
})

# Plots
plot(dsm[[16]]); plot(chm[[16]]) #note the scale bar differences


```

## Individual tree detection

Individual tree detection function that find the position of the trees using several possible algorithms.

```{r}
#| label: locate_trees
#| eval: !expr params$rundataproducts

help(locate_trees)

```

```{r}
#| label: individual tree detection
#| eval: !expr params$rundataproducts
#| webgl: true

# Detect trees
f_find_trees <- function(x) {
  y <- 3
  y[x < 0.1] <- 0.1
  y[x < 3 & x >= 0.1] <- 1
  y[x >= 3] <- 3
  return(y)
}

ttops <- lapply(1:nrow(receiver_loc), function(rcvr){
  locate_trees(norm_height[[rcvr]],  lmf(ws=f_find_trees))
})

# Plot an example
plot(soundsites_transects[[16]]);rglwidget()

plot(chm[[16]], col = height.colors(50))
plot(sf::st_geometry(ttops[[16]]), add = TRUE, pch = 3)


# Segment trees
algo <-
lapply(1:nrow(receiver_loc), function(rcvr){
  dalponte2016(chm[[rcvr]], ttops[[rcvr]])
})

trees_seg <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
  segment_trees(norm_height[[rcvr]], algo[[rcvr]])
})

plot(trees_seg[[16]], bg = "white", size = 4, color = "treeID");rglwidget() # visualize trees

tree.h <- 
lapply(1:nrow(receiver_loc), function(rcvr){
  list(
 count.trees = sum(unique(trees_seg[[rcvr]]@data$treeID), na.rm = T),
 height.mean.trees = mean(trees_seg[[rcvr]]@data$Z, na.rm = T)
)
})

tree.h <- data.table::rbindlist(tree.h, fill=TRUE)
tree.h[tree.h$count.trees == 0, "height.mean.trees"] <- 0

# combine with magnumopus
magnumopus <- cbind(magnumopus, tree.h)

# trees per m2
magnumopus$count.trees.m2 <- magnumopus$count.trees / magnumopus$transect_area


# Crown metrics
crowns <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
  tryCatch(
    crown_metrics(trees_seg[[rcvr]], func = .stdtreemetrics, geom = "convex"),
           error = function(e) return(NULL)
    )
})

plot(crowns[[16]]["convhull_area"])

crowns_area <- 
  lapply(1:nrow(receiver_loc), function(rcvr){
  tryCatch(
    sum(crowns[[rcvr]]$convhull_area),
           error = function(e) return(NULL)
    )
})
crowns_area <- do.call(rbind,crowns_area)

# combine
magnumopus <- cbind(magnumopus,crowns_area)

magnumopus$crown_area_m2 <- magnumopus$crowns_area / magnumopus$transect_area
magnumopus$n_area_m2 <- magnumopus$n / magnumopus$transect_area

rm(f_find_trees, ttops, algo, trees_seg, tree.h, crowns, crowns_area); gc()
```

```{r}
#| label: write main dt to disk
#| eval: !expr params$rundataproducts

write.csv(magnumopus, here::here("data_output","products","sqrl_sound_ldscp_complete.csv"))
head(magnumopus)

# clean up
rm(magnumopus); gc()

```

This is the end of the `rundataproducts` section.

# Load data

Here we load the big data frame with all the generated data products.

```{r}
#| label: load magnumopus

# Load the main data frame again if you do not want to run all the above chunks
magnumopus <- read.csv(here::here("data_output","products","sqrl_sound_ldscp_complete.csv"), header = T)

```

We can now merge the big data frame with other data products loaded from file.

```{r}
#| label: merge datasets

# read other data from file.
playb <- read.csv(here::here("data","MGRS_playbacks_fuzzy.csv"))
dim(playb) # always check if the dimensions are as expected
head(playb) # lets have a look

# Here we drop some variables, rename variables to match between data sets, mutate a variable from a numeric variable to a character variable, distance to factorial, and merge the additional data with the main data set using the plot attributes.
sound <- 
playb %>%
  dplyr::select(-n,-sd,-X) %>%
  dplyr::rename(plot = Plot,
                cardinal.direction = Cardinal.Direction,
                distance = Distance..m.,
                treatment = Treatment,
                alivebaperha = AliveBAperha) %>%
  dplyr::mutate(bearing = if_else(cardinal.direction == "N", 0,
                          if_else(cardinal.direction == "E", 90,
                          if_else(cardinal.direction == "S", 180, 270))),
                fac.distance = as.factor(distance)) %>%
  dplyr::left_join(magnumopus, by = join_by(plot == plot, treatment == treatment,
                                            distance == distance, bearing == bearing))

# check merge
dim(sound) # check dimensions to make sure it matches 
head(sound) # let's have a look
tail(sound)

# Fix incompatible column names
names(sound) <- str_replace(names(sound), "[-]", "_") # From - to _

# write to disk
write.csv(sound, here::here("data_output","products","sound.csv"))

# clean up
rm(playb, sound); gc()
#plan(multisession, workers = 4L)
```

# Analysis

We can now start statistical analysis.

```{r}
#| label: load sound again

sound <- read.csv(here::here("data_output","products","sound.csv"), header = T)
sound$fac.distance <- as.factor(sound$fac.distance)

head(sound)

```

```{r}
#| label: Exploratory plots

# Exploratory plot
plot_amp_treat_dist <-
ggplot(data = sound, aes(y = amp, x = treatment, color = treatment)) +
  geom_boxplot() +
  geom_point() +
  facet_grid(~fac.distance) +
  ylab("Amplitude") +
  scale_color_discrete(name = "Treatment") +
  scale_shape_discrete(name = "Treatment") +
  theme_classic()

plot_amp_treat_dist + plot_layout(guides = 'collect')
  
```

## Model 0

Initial model with `distance`.

```{r}
#| label: mod0

# Initial models
mod0 <- lmer(amp ~ 1 + 
             fac.distance +
             (1|plot),
             data = sound[sound$distance != 1,])
#summary(mod0)
```

```{r}
#| label: mod0_tab

tab_model(mod0, auto.label = TRUE, show.reflvl = F)

```

```{r}
#| label: mod0_plot

plot_model(mod0, type = "pred", terms = c("fac.distance")) +
plot_model(mod0, type = "re")

```

## Model 1

Adding `treatment`.

```{r}
#| label: mod1

mod1 <- lmer(amp ~ 
               fac.distance +
               treatment +
               (1|plot),
             data = sound[sound$distance != 1,])
#summary(mod1)
anova(mod0, mod1)

```

Treatment effect add significant effect to the model.

```{r}
#| label: mod1_tab

tab_model(mod0, mod1, auto.label = TRUE, show.reflvl = F)

```

```{r}
#| label: mod1_plot

plot_model(mod1, type = "pred", terms = c("treatment", "fac.distance"))

```

## Model 2

Adding `CRR`

```{r}
#| label: mod2

mod2 <-  lmer(amp ~ 
                fac.distance +
                treatment +
                CRR +
             (1|plot),
             data = 
               sound[sound$distance != 1,],
             na.action = na.fail)
summary(mod2)
MuMIn::r.squaredGLMM(mod2)
anova(mod1, mod2) # not significant

plot_model(mod2, type = "pred", terms = c("CRR [all]")) +
plot_model(mod2, type = "re") 


```

## Model 3

```{r}
#| label: mod4

mod3 <- lmer(amp ~ 
             fac.distance +
             treatment +
             CRR +
             (1|plot),
             data = sound[sound$distance >= 10,])
summary(mod3)
#plot_model(mod3, type = "pred", terms = c("CRR [all]")) 
#MuMIn::r.squaredGLMM(mod3)
mod3_rtable <- tab_model(mod3, auto.label = TRUE, show.reflvl = F) # model results table 
#plot_model(mod3, type = "pred", terms = c("CRR [all]")) 

# Generate conditional predictions
mod3.ggeffect <- as.data.frame(ggeffect(mod3, type = "fixed", terms = c("CRR [all]"))) 

mod3.ggeffect.distance <- as.data.frame(ggeffect(mod3, type = "fixed", terms = c("fac.distance")))

mod3.ggeffect.treatment <- as.data.frame(ggeffect(mod3, type = "fixed", terms = c("treatment"))) 

# Treatment plot
plot.mod3.ggeffect <-
ggplot(data = mod3.ggeffect, aes(x = x, y = predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha=0.2) +
  xlab("CRR") +
  ylab("Amplitude") +
  #labs(title = "Predicted amplitude",
  #     subtitle = paste0("Predicted amplitude at 23 meters. Canopy Relief Ratio (CRR) at the mean and 1 standard deviation.")) +
  theme_classic()

# Distance plot
plot.mod3.ggeffect.distance <-
ggplot(data = mod3.ggeffect.distance, aes(x = x, y = predicted)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  xlab("Distance (m)") +
  ylab("Amplitude") +
  #labs(title = "Predicted amplitude") +
  theme_classic()

# Treatment plot
plot.mod3.ggeffect.treatment <-
ggplot(data = mod3.ggeffect.treatment, aes(x = x, y = predicted)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  xlab("Treatment") +
  ylab("Amplitude") +
  #labs(title = "Predicted amplitude") +
  theme_classic()

plot_main_mod3 <-
plot.mod3.ggeffect.treatment + 
  plot.mod3.ggeffect.distance + 
   plot.mod3.ggeffect +
plot_annotation(
    title = 'Predicted amplitude',
    tag_levels = 'a')

```

```{r mod3_table}
#| label: mod3_table

mod3_rtable
```

```{r}
#| label: mod3_plot

plot_main_mod3
```

Here we plot a cross-section of a plot with low and high mean CRR values.

```{r}
#| label: CRRlowhigh

CRRlowhigh <-
sound %>%
  group_by(plot) %>%
  dplyr::summarize(crr = mean(CRR, na.rm = T)) %>%
  arrange(crr) 

xhigh <- CRRlowhigh %>% top_n(2)
xlow <- CRRlowhigh %>% slice(4:5)

#rm(CRRlowhigh)
```

```{r}
#| label: CRR plot
#| include: false

#low
low1 <- readLAS(here::here("data_output","pointcloud","verdekaibab_2018_transects_norm", paste0(xlow[1,1],"_40_90.las")))	
low2 <- readLAS(here::here("data_output","pointcloud","verdekaibab_2018_transects_norm",  paste0(xlow[2,1],"_40_90.las")))	

#high
high1 <- readLAS(here::here("data_output","pointcloud","verdekaibab_2018_transects_norm",  paste0(xhigh[1,1],"_40_90.las")))
high2 <- readLAS(here::here("data_output","pointcloud","verdekaibab_2018_transects_norm",  paste0(xhigh[2,1],"_40_90.las")))


plot_low <- plot_crossection(low1,
                             colour_by = Z,
                             Clip_Transect = FALSE) +
   ylim(0,30) +
   ggtitle(paste0("CCR = ", round(xlow[1,2],3))) +
   ylab("Height (m)") +
   xlab("Longitude") +
   theme_classic() +
   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

plot_low_2 <- plot_crossection(low2,
                             colour_by = Z,
                             Clip_Transect = FALSE) +
   ylim(0,30) +
   ggtitle(paste0("CCR = ", round(xlow[2,2],3))) +
   ylab("Height (m)") +
   xlab("Longitude")  +
   theme_classic() +
   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

plot_high <- plot_crossection(high1,
                              colour_by = Z,
                              Clip_Transect = FALSE) +
  ylim(0,30) +
  ggtitle(paste0("CCR = ", round(xhigh[1,2],3))) +
  ylab("Height (m)") +
  xlab("Longitude")  +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

plot_high_2 <- plot_crossection(high2,
                              colour_by = Z,
                              Clip_Transect = FALSE) +
  ylim(0,30) +
  ggtitle(paste0("CCR = ", round(xhigh[2,2],3))) +
  ylab("Height (m)") +
  xlab("Longitude")  +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

ccr_p1 <- plot_low + plot_low_2 
ccr_p2 <- plot_high + plot_high_2

plot_ccr_example <-
(ccr_p1) /
(ccr_p2) +
  plot_annotation(tag_levels = 'a') +
  plot_layout(guides='keep')

rm(plot_low, plot_low_2, plot_high, plot_high_2,
   ccr_p1, ccr_p2,
   CRRlowhigh)
gc()

```

```{r}
#| label: plot_crr_example
#| echo: false

plot_ccr_example

```




